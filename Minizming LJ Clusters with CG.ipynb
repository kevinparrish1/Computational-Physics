{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lennard-Jones Potential\n",
    "<br>\n",
    "\n",
    "\n",
    "$$V(r)=4\\epsilon\\big[\\big(\\frac{\\delta}{r}\\big)^{12}-\\big(\\frac{\\delta}{r}\\big)^6\\big]$$\n",
    "<br>\n",
    "<br>\n",
    "Suppose we have $m$ molecules whose positions are denoted by $\\textbf{a}_m$ that exist in $\\mathbb{R^n}$. Let $\\textbf{A}$ be an $m\\times{n}$ matrix that contains the molecules, and let $\\textbf{r}$ be a $m\\times{m}$ square symetric matrix where $r_{ij}=\\|\\textbf{A}_i-\\textbf{A}_j\\|$ \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textbf{A}&=\\left[\\begin{matrix} \\textbf{a}_1\\\\\\textbf{a}_2\\\\\\vdots\\\\\\textbf{a}_m\\end{matrix}\\right]=\n",
    "\\left[\\begin{matrix} a_{1_1} & a_{1_2} & \\dots & a_{1_n}\\\\ a_{2_1} & \\ddots & & \\\\\\vdots & & \\ddots &\\\\a_{m_1} & & & a_{m_n}\\end{matrix}\\right]\\\\\n",
    "\\textbf{r}&=\\left[\\begin{matrix} \\|\\textbf{A}_1-\\textbf{A}_1\\| & \\dots & \\|\\textbf{A}_1-\\textbf{A}_j\\| \\\\ \\vdots &\\ddots & \\\\ \\|\\textbf{A}_i-\\textbf{A}_1\\| & & \\|\\textbf{A}_m-\\textbf{A}_m\\| \\end{matrix}\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "The total energy $E$ of a system $\\textbf{A}$ with a distance matrix $\\textbf{r}$ is defined as the sum of the LJ energy between every molecule.\n",
    "<br>\n",
    "<br>\n",
    "$$E(\\textbf{A})=\\sum_{i=0}^{m-1}\\sum_{j=i+1}^{m}V(r_{ij})\\quad {\\scriptsize \\textrm{in total, } {m\\choose 2 } {\\textrm{ terms. (upper triangle of }}\\textbf{r})}\n",
    "$$\n",
    "\n",
    "For computational purposes, only the upper triangle of $\\textbf{r}$ is calculated, and it is flattened to 1 dimension. The final energy calculation then only needs one iterator over the span of $\\textbf{r}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def Energy(A,dim):\n",
    "    def V(r,epsilon=1,delta=1):#Returns the potential between two molecules of distance r\n",
    "        return 4*epsilon*((delta/r)**12 - (delta/r)**6)\n",
    "    \n",
    "    def r(A):#Returns a distance matrix. Only the upper triangle needs to be calculated; the rest can be 0\n",
    "        r=[]\n",
    "        for i in range(0,A.shape[0]-1):\n",
    "            for j in range(i+1,A.shape[0]):\n",
    "                r.append(np.sqrt(np.dot(A[i]-A[j],A[i]-A[j])))\n",
    "        return np.asarray(r)\n",
    "    A=np.reshape(A,(int(len(A)/dim),dim))\n",
    "    r1=r(A)\n",
    "    total=0.\n",
    "    for i in range(len(r1)):\n",
    "            total+=V(r1[i])\n",
    "    return total.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lennard Jones Force Function\n",
    "\n",
    "<br>\n",
    "\n",
    "Lets examine the force vector $\\textbf{F}$ on a location $\\textbf{x}$ generated from a particle $\\textbf{a}$ in $\\mathbb{R^n}$ space. Let $(x_1,x_2,\\dots,x_n)$ denote the basis vectors. <br>\n",
    "For simplicity,  $\\epsilon,\\delta$ =1.\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textbf{F}(\\textbf{x})&=\\nabla V\\\\\n",
    "&=4\\nabla\\big[\\big(\\frac{1}{r}\\big)^{12}-\\big(\\frac{1}{r}\\big)^6\\big]\\\\\n",
    "&= \\big[-\\frac{48}{r^{13}}+\\frac{24}{r^7}\\big]\\nabla r\\\\\n",
    "&= \\big[-\\frac{48}{r^{13}}+\\frac{24}{r^7}\\big]\\big(\\frac{\\partial}{\\partial x_1}\\hat x_1+\\frac{\\partial}{\\partial x_2}\\hat x_2+\\dots+\\frac{\\partial}{\\partial x_n}\\hat x_n\\big)\\big((a_1-x_1)^2+(a_2-x_2)^2+\\dots+(a_n-x_n)^2\\big)^{\\frac{1}{2}}\\\\\n",
    "&= \\big[\\frac{48}{r^{14}}-\\frac{24}{r^8}\\big]\\big((a_1-x_1)\\hat x_1+(a_2-x_2)\\hat x_2 + \\dots + (a_n-x_n)\\hat x_n\\big)\\\\\n",
    "&= \\big[\\frac{48}{r^{14}}-\\frac{24}{r^8}\\big](\\textbf{a}-\\textbf{x})\n",
    "\\end{align*}\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "Due to the principle of superposition, we can easily extrapolate to include $m$ molecules\n",
    "<br>\n",
    "<br>\n",
    "$$\\textbf{F}(\\textbf{x})=\\sum_{i=1}^{m}\\big[\\frac{48}{r_i^{14}}-\\frac{24}{r_i^8}\\big](\\textbf{a}_i-\\textbf{x})\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "If this force function was evaluated at every particle location $\\textbf{a}_m$, you would have a complete force matrix. The direction of greatest increase for each particle would be specifically calculated based on the energy of the system $\\textbf{A}$, and each location parameter $a_{mn}$ would be automatically filled in by the gradient. Thus,\n",
    "\n",
    "$$\\nabla E=\\left[\\begin{matrix}\\textbf{F}(\\textbf{a}_1) \\\\ \\textbf{F}(\\textbf{a}_2) \\\\ \\vdots \\\\ \\textbf{F}(\\textbf{a}_m)\\end{matrix}\\right]\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "Within the evaluation of $\\textbf{F}$, the contribution of a particle onto itself must manually removed, or else $r$ calculation will yield a divide by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientE(A,dim):\n",
    "    def F(A,x):\n",
    "        total=np.zeros([len(x)])\n",
    "        for i in range(A.shape[0]):\n",
    "            r=np.sqrt(np.dot(A[i]-x,A[i]-x))\n",
    "            k=(48/r**14)-(24/r**8)\n",
    "            total+=k*(A[i]-x)\n",
    "        return total\n",
    "\n",
    "    A=np.reshape(A,(int(len(A)/dim),dim))\n",
    "    force=np.zeros(A.shape)\n",
    "    for i in range(A.shape[0]):\n",
    "        A1=A.copy()\n",
    "        A1=np.delete(A1,i,0)\n",
    "        force[i]=F(A1,A[i])\n",
    "\n",
    "    return force.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Error of LJ-Potential:  1.0015150846790183e-13\n",
      "Percentage Error of LJ-Force:  3.5673908814841597e-12\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Check\n",
    "from LJ import LJ,LJ_force\n",
    "import numpy as np\n",
    "def init(m,n):#Returns a random, normalized array of size mxn\n",
    "    A=np.random.uniform(low=-1.,high=1.,size=(m,n))\n",
    "    return A\n",
    "def error(x,y): #I'm using the LJ from your optimization repository as the theoretical\n",
    "    return (100*(x-y)/x)\n",
    "energy=0.\n",
    "gradient=0.\n",
    "for i in range(100):#Average the error over 100 random systems of A\n",
    "    A=init(3,3)\n",
    "    energy+=np.abs(error(Energy(A.flatten(),3),LJ(A.flatten(),3)))\n",
    "    gradient+=np.abs(error(np.linalg.norm(gradientE(A.flatten(),3)),np.linalg.norm(LJ_force(A.flatten(),3))))\n",
    "energy=float(energy/100.)\n",
    "graident=gradient/100\n",
    "print('Percentage Error of LJ-Potential: ',energy)\n",
    "print('Percentage Error of LJ-Force: ', gradient)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradient for Non-Linear Systems\n",
    "<br>\n",
    "\n",
    "$\\text{Algorithm detailed in }\\small\\textit{Numerical Optimization} \\text{ by Nocedal & Wright.}$\n",
    "### Conjugate Gradient\n",
    "$\\text{Given } x_0;$<br>\n",
    "$\\text{Evalute } f_0=f(x_0), \\nabla f_0= \\nabla f(x_0);$<br>\n",
    "$\\text{Set }p_0=- \\nabla f_0, k\\leftarrow0$<br>\n",
    "$\\begin{align*} \\textbf{while } \\nabla &f_k \\neq 0 \\\\\n",
    "&\\text{Compute } \\textbf{LineSearch}(\\alpha_k) \\text{ and set }x_{k+1} = x_k + \\alpha_kp_k;\\\\\n",
    "&\\text{Evalute } \\nabla f_{k+1}; \\\\\n",
    "\\beta_{k+1}^{\\text{PR}} &\\leftarrow \\text{max}\\left(\\frac{(\\nabla f_{k+1}-\\nabla f_{k})^T \\nabla f_{k+1}}{\\nabla f_k^T \\nabla f_k},0\\right);\\\\\n",
    "p_{k+1} &\\leftarrow -\\nabla f_{k+1} + \\beta_{k+1}^\\text{PR}p_k;\\\\\n",
    "k &\\leftarrow k + 1\n",
    "\\end{align*}$\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Parameters\n",
    "-----------\n",
    "f : function to be evaluated. First argument must be x0\n",
    "x0 : initial guess of parameters begin the iterative algorithm\n",
    "     should be preconditioned to not be in an ill-behaving region of f for ideal performance\n",
    "fprime : analytical gradient of f. If none is given defaults to using a two point finite difference method\n",
    "args : arguments to pass into f and fprime\n",
    "gtol : tolerance of the largest derivative of all the parameters before the algorithm has converged to local minimum\n",
    "maxiter : maximum number of steps the algorithm will take\n",
    "eps : step sized used calculate the finite difference method if no analytical gradient is given\n",
    "\n",
    "\n",
    "Returns [fval,xk,gnorm]\n",
    "----------------------\n",
    "fval : the minimum value of the function\n",
    "xk : the minimizer of f\n",
    "gnorm : the largest partial derivative value of all the parameters\n",
    "\"\"\"\n",
    "\n",
    "def CG(f,x0,fprime=None,args=(),gtol=1e-5,maxiter=None,eps = np.sqrt(np.finfo(float).eps)):\n",
    "\n",
    "    \n",
    "    def approx_jac(A,dim): #replaces fprime if none is given. Uses two point finite difference method\n",
    "        fprime=np.zeros([len(A)])\n",
    "        for i in range(len(A)):\n",
    "            delta=np.zeros([len(A)])\n",
    "            delta[i]=eps\n",
    "            fprime[i]=(f(A+delta)-f(A-delta))/(2*eps)\n",
    "        return fprime\n",
    "    \n",
    "    \n",
    "    def wrap_function(function, args):#wraps arguments into the function\n",
    "        ncalls = [0]\n",
    "        def function_wrapper(*wrapper_args):\n",
    "            ncalls[0] += 1\n",
    "            return function(*(wrapper_args + args))\n",
    "\n",
    "\n",
    "        return ncalls, function_wrapper\n",
    "    \n",
    "    \n",
    "    func_calls, f = wrap_function(f, args)\n",
    "    if fprime==None:\n",
    "        grad_calls,fprime=wrap_function(approx_jac,args)\n",
    "    else:\n",
    "        grad_calls,fprime=wrap_function(fprime, args)\n",
    "    \n",
    "    \n",
    "    if maxiter is None:\n",
    "        maxiter = len(x0) * 200 #The artificial maximum number of iterations\n",
    "\n",
    "    xk=x0\n",
    "    gfk=fprime(xk)\n",
    "    pk=-gfk\n",
    "    old_fval = f(xk) #Artificially generates previous steps to help the line search run smoother\n",
    "    old_old_fval = old_fval + np.linalg.norm(gfk) / 2\n",
    "    k=0\n",
    "    gnorm = np.amax(np.abs(gfk))\n",
    "    cached_step = [None]#holds caluclated values to prevent redundant computation\n",
    "    \n",
    "    \n",
    "    while(gnorm>gtol) and (k<maxiter):\n",
    "        \n",
    "        \n",
    "        def step(alpha,gfk1=None):#A step length using the polak-ribiere beta value\n",
    "            xk1 = xk + alpha * pk\n",
    "            if gfk1 is None:\n",
    "                gfk1=fprime(xk1)\n",
    "            betak1 = max(0, np.dot((gfk1-gfk), gfk1) / np.dot(gfk,gfk))\n",
    "            pk1 = -gfk1 + betak1 * pk\n",
    "            gnorm = np.amax(np.abs(gfk1))\n",
    "            return (alpha, xk1, pk1, gfk1, gnorm)\n",
    "        \n",
    "        \n",
    "        def descent_condition(alpha, gfk1=None):#Using the polak-ribiere step length, pk is not guarenteed to satisfy\n",
    "            cached_step[:] = step(alpha, gfk1)  #sufficient decrease condition. Must test for it.\n",
    "            alpha, xk, pk, gfk,gnorm=step(alpha,gfk1)\n",
    "            if gnorm<gtol:\n",
    "                return True\n",
    "            return np.dot(pk, gfk) <= -.01 * np.dot(gfk, gfk)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            alpha_k, fc, gc, old_fval, old_old_fval, gfk1 = \\\n",
    "                                    LineSearchHub(f, fprime, xk, pk, gfk, old_fval,\n",
    "                                          old_old_fval, c2=0.4, amin=1e-100, amax=1e100,\n",
    "                                          extra_condition=descent_condition)\n",
    "        except _LineSearchError:\n",
    "            break\n",
    "\n",
    "\n",
    "        if alpha_k == cached_step[0]:\n",
    "            alpha_k, xk, pk, gfk, gnorm = cached_step\n",
    "        else:\n",
    "            alpha_k, xk, pk, gfk, gnorm = step(alpha_k, gfk1)\n",
    "        k += 1\n",
    "    \n",
    "    fval=old_fval\n",
    "    return[fval,xk,gnorm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minpack2\n",
    "from scipy.optimize.linesearch import line_search_wolfe1\n",
    "import warnings\n",
    "from scipy.optimize.linesearch import LineSearchWarning\n",
    "\n",
    "class _LineSearchError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "LineSearchHub Parameters\n",
    "------------------------\n",
    "f : function to be evaluated. First argument must be x0\n",
    "fprime : gradient of f\n",
    "xk= the fixed point on the line to be minimized\n",
    "pk : the direction of the line\n",
    "gfk : fprime evaluated at xk\n",
    "old_fval : function value at previous point\n",
    "old_old_fval : function value at previous previous point\n",
    "c2 : constant for determining curvature condition\n",
    "amin;amax : min and max alpha values\n",
    "extra_condition : required condition that the new alpha must satsify. Has arguments of form (alpha, gfkp1)\n",
    "                  where gfkp1 is the gradient at the new point proposed by alpha\n",
    "\n",
    "\n",
    "Returns [fval,xk,gnorm]\n",
    "----------------------\n",
    "fval : the minimum value of the function\n",
    "xk : the minimizer of f\n",
    "gnorm : the largest partial derivative value of all the parameters\n",
    "\"\"\" \n",
    "    \n",
    "    \n",
    "def LineSearchHub(f, fprime, xk, pk, gfk, old_fval, old_old_fval,\n",
    "                     **kwargs):\n",
    "\n",
    "\n",
    "    extra_condition = kwargs.pop('extra_condition', None)#the 1st line search method doesn't have an extra condition as a parameter\n",
    "\n",
    "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
    "                             old_fval, old_old_fval,\n",
    "                             **kwargs)\n",
    "\n",
    "    if ret[0] is not None and extra_condition is not None:\n",
    "        xp1 = xk + ret[0] * pk\n",
    "        if not extra_condition(ret[0],ret[5]):\n",
    "            ret = (None,)\n",
    "\n",
    "    if ret[0] is None:#This is the pure pythonic line search that is used if 1st method fails to converge.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', LineSearchWarning)\n",
    "            kwargs2 = {}\n",
    "            for key in ('c1', 'c2', 'amax'):\n",
    "                if key in kwargs:\n",
    "                    kwargs2[key] = kwargs[key]\n",
    "            ret = search(f, fprime, xk, pk, gfk,\n",
    "                                     old_fval, old_old_fval,\n",
    "                                     extra_condition=extra_condition,\n",
    "                                   **kwargs2)\n",
    "\n",
    "\n",
    "\n",
    "    if type(ret) is not tuple:\n",
    "        raise _LineSearchError()\n",
    "    if ret[0] is None:\n",
    "        raise _LineSearchError()\n",
    "    return ret\n",
    "    \n",
    "def zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo,\n",
    "              phi, derphi, phi0, derphi0, c1, c2, extra_condition): #Given an interval of alpha values, finds the minimum\n",
    "        maxiter = 100\n",
    "        i = 0\n",
    "        delta1 = 0.2  # cubic interpolant check\n",
    "        delta2 = 0.1  # quadratic interpolant check\n",
    "        phi_rec = phi0\n",
    "        a_rec = 0\n",
    "        while True:\n",
    "#             print([a_lo,a_hi])\n",
    "            # interpolate to find a trial step length between a_lo and\n",
    "            # a_hi Need to choose interpolation here.  Use cubic\n",
    "            # interpolation and then if the result is within delta *\n",
    "            # dalpha or outside of the interval bounded by a_lo or a_hi\n",
    "            # then use quadratic interpolation, if the result is still too\n",
    "            # close, then use bisection\n",
    "\n",
    "            dalpha = a_hi - a_lo\n",
    "            if dalpha < 0:#Orders the interval boundaries\n",
    "                a, b = a_hi, a_lo\n",
    "            else:\n",
    "                a, b = a_lo, a_hi\n",
    "\n",
    "\n",
    "            if (i > 0):#Cannot lead with a cubic interpolation because the formula requires a previous iteration.\n",
    "                cchk = delta1 * dalpha #The distance the minimum must be from either endpoint (20% of the range)\n",
    "                a_j = _cubicmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi,\n",
    "                                a_rec, phi_rec)\n",
    "                \n",
    "            if (i == 0) or (a_j is None) or (a_j > b - cchk) or (a_j < a + cchk): #runs in 1st iteration, or if cubic fails\n",
    "                qchk = delta2 * dalpha #quadratic interpolation shouldn't be within 10% of the range from the boundaries\n",
    "                a_j = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n",
    "                if (a_j is None) or (a_j > b-qchk) or (a_j < a+qchk):\n",
    "                    a_j = a_lo + 0.5*dalpha  #initializes a_j for a golden section method if both cubic and quadratic fail\n",
    "\n",
    "\n",
    "            phi_aj = phi(a_j)\n",
    "            if (phi_aj > phi0 + c1*a_j*derphi0) or (phi_aj >= phi_lo): #Tests Wolfe conditions 1. If fails, will reset boundaries\n",
    "                phi_rec = phi_hi                                       #and try again\n",
    "                a_rec = a_hi\n",
    "                a_hi = a_j\n",
    "                phi_hi = phi_aj\n",
    "            else:\n",
    "                derphi_aj = derphi(a_j) #Tests Wolfe conditions 2. If passes, a_j is a good step length, loop breaks.\n",
    "                if abs(derphi_aj) <= -c2*derphi0 and extra_condition(a_j,None):\n",
    "                    a_star = a_j\n",
    "                    val_star = phi_aj\n",
    "                    valprime_star = derphi_aj\n",
    "                    break\n",
    "                if derphi_aj*(a_hi - a_lo) >= 0:\n",
    "                    phi_rec = phi_hi\n",
    "                    a_rec = a_hi\n",
    "                    a_hi = a_lo\n",
    "                    phi_hi = phi_lo\n",
    "                else:\n",
    "                    phi_rec = phi_lo\n",
    "                    a_rec = a_lo\n",
    "                a_lo = a_j\n",
    "                phi_lo = phi_aj\n",
    "                derphi_lo = derphi_aj\n",
    "            i += 1\n",
    "            if (i > maxiter):\n",
    "                a_star = None\n",
    "                val_star = None\n",
    "                valprime_star = None\n",
    "                break\n",
    "        return a_star, val_star, valprime_star\n",
    "\n",
    "\n",
    "def _cubicmin(a, fa, fpa, b, fb, c, fc): #Uses the known minimum of a cubic polynomial that passes through the boundaries. \n",
    "                                         #Requires a previous step\n",
    "\n",
    "    # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D\n",
    "\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            C = fpa\n",
    "            db = b - a\n",
    "            dc = c - a\n",
    "            denom = (db * dc) ** 2 * (db - dc)\n",
    "            d1 = np.empty((2, 2))\n",
    "            d1[0, 0] = dc ** 2\n",
    "            d1[0, 1] = -db ** 2\n",
    "            d1[1, 0] = -dc ** 3\n",
    "            d1[1, 1] = db ** 3\n",
    "            [A, B] = np.dot(d1, np.asarray([fb - fa - C * db,\n",
    "                                            fc - fa - C * dc]).flatten())\n",
    "            A /= denom\n",
    "            B /= denom\n",
    "            radical = B * B - 3 * A * C\n",
    "            xmin = a + (-B + np.sqrt(radical)) / (3 * A)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin    \n",
    "    \n",
    "    \n",
    "    \n",
    "def _quadmin(a, fa, fpa, b, fb): #Uses the known minium of a quadratic poylnomial that passes through the boundaries.\n",
    "    \n",
    "    # f(x) = B*(x-a)^2 + C*(x-a) + D\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            D = fa\n",
    "            C = fpa\n",
    "            db = b - a * 1.0\n",
    "            B = (fb - D - C * db) / (db * db)\n",
    "            xmin = a - C / (2.0 * B)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "    \n",
    "def search(f,fprime,xk,pk,gfk,old_fval,old_old_fval,c1=1e-4, c2=0.4, amin=1e-100, amax=1e100, maxiter=100, extra_condition=None):\n",
    "    #tries to establish an interval containing the minimizer. If found, calls on the zoom function to return the best step size\n",
    "    fc=0\n",
    "    gc=0\n",
    "    def phi(alpha):\n",
    "        if alpha!=None:\n",
    "            return f(xk+alpha*pk)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def phiprime(alpha):\n",
    "        return np.dot(fprime(xk+alpha*pk),pk)\n",
    "\n",
    "\n",
    "\n",
    "    if extra_condition is None:\n",
    "        def extra_condition(a,b):\n",
    "            return True\n",
    "\n",
    "    alpha=np.zeros([2])\n",
    "    if phiprime(0.)!=0:\n",
    "        alpha[1]=min(1.,1.01*2*(old_fval-old_old_fval)/phiprime(0.))#Ensures superlinear convergence rate \n",
    "    else:\n",
    "        alpha[1]=1.\n",
    "    if alpha[1]<0.:\n",
    "        alpha[1]=1.\n",
    "    i=0\n",
    "    phiprime0=phiprime(0.)\n",
    "    phi0=phi(0.)\n",
    "    \n",
    "    while i<maxiter:\n",
    " \n",
    "        phik=phi(alpha[1])\n",
    "        if ((phik>(phi0+c1*alpha[1]*phiprime0)) or ((phik >=phi(alpha[0])) and i>1)):#tests wolfe conditions 1\n",
    "            alphastar=zoom(alpha[0],alpha[1],phi(alpha[0]),phi(alpha[1]),phiprime(alpha[0]),phi,phiprime,phi0,phiprime0,c1,c2,extra_condition)[0]\n",
    "            return alphastar,fc,gc,phi(alphastar),old_fval,None\n",
    "\n",
    "        phikprime=phiprime(alpha[1]) #tests wolfe conditions 2\n",
    "        if np.abs(phikprime)<=-c2*phiprime0:\n",
    "            if extra_condition(alpha[1],None):\n",
    "                alphastar=alpha[1]\n",
    "                return alphastar,fc,gc,phi(alphastar),old_fval,None\n",
    "\n",
    "        if phikprime>=0: #if the gradient of the new test alpha is positive and failed wolfe 2, it means the minimizer is in the range, but the boundaries are flipped\n",
    "            alphastar=zoom(alpha[1],alpha[0],phi(alpha[1]),phi(alpha[0]),phiprime(alpha[1]),phi,phiprime,phi0,phiprime0,c1,c2,extra_condition)[0]\n",
    "            return alphastar,fc,gc,phi(alphastar),old_fval,None\n",
    "        alpha2=1.1*alpha[1]\n",
    "        alpha[0]=alpha[1]\n",
    "        alpha[1]=alpha2\n",
    "        i+=1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Test\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "t1=[]\n",
    "t2=[]\n",
    "dim=3\n",
    "nmin=2\n",
    "nmax=20\n",
    "for i in np.arange(nmin,nmax,1):\n",
    "    A=init(i,dim)\n",
    "    start=time.time()\n",
    "    CG(Energy,A.flatten(),args=(dim,))\n",
    "    end=time.time()\n",
    "    t1.append(end-start)\n",
    "    start=time.time()\n",
    "    CG(Energy,A.flatten(),args=(dim,),fprime=gradientE)\n",
    "    end=time.time()\n",
    "    t2.append(end-start)\n",
    "t1=np.asarray(t1)\n",
    "t2=np.asarray(t2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x=np.arange(nmin,nmax,1)\n",
    "fig=plt.figure()\n",
    "ax1=fig.add_subplot(111)\n",
    "ax1.plot(x,t1/t2)#The y axis is the speed of the approximated gradient divided by the speed of the analytical gradient.\n",
    "ax1.set_xlabel('Number of Molecules')\n",
    "ax1.set_ylabel('Speed Ratio')\n",
    "ax1.set_title('My CG')\n",
    "plt.show()\n",
    "print('With the analytical gradient, the speed is approximately 2n faster where n is the number of molecules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999\n",
      "0.999\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "import myscipy_optimize\n",
    "q=0\n",
    "t=0\n",
    "n=1000\n",
    "for i in range(n):\n",
    "    A=init(3,3).flatten()\n",
    "    while(Energy(A,3)>100):\n",
    "        A=init(3,3).flatten()\n",
    "    if CG(Energy,A,fprime=gradientE,args=(3,))[0]<-2.99:\n",
    "        q+=1\n",
    "    if scipy.optimize.minimize(Energy,A,jac=gradientE,args=(3,),method='cg').fun<-2.99:\n",
    "        t+=1\n",
    "print(q/n)\n",
    "print(t/n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
